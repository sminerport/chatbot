{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"flask-chatterbot-simple.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1kCQkP7eEvxrpkp79sfdkAVyIKw9jSugC","authorship_tag":"ABX9TyPDebL4SQLvZ52mLd73Gim4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AC9_kW-ZlS6W","collapsed":true,"executionInfo":{"status":"ok","timestamp":1633769088079,"user_tz":360,"elapsed":4023,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"e978d321-f258-4189-8ad4-93b8b777838b"},"source":["!pip install flask-ngrok"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","metadata":{"id":"WQv6NX-YlnAg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e863f91-ed2e-4b9d-e7da-037a916d6b65"},"source":["from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request\n","import re\n","import os\n","from time import time\n","\n","import numpy as np\n","import pandas as pd\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from keras.layers import Dense, Input, LSTM, Embedding, RepeatVector, concatenate, TimeDistributed\n","from keras.models import Model\n","from keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","from keras.utils import np_utils\n","from nltk.tokenize import casual_tokenize\n","import joblib\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","import pickle\n","import numpy as np\n","from keras.models import load_model\n","\n","import json\n","import random\n","\n","# text clean up imports\n","import textwrap\n","import nltk.data\n","\n","# fold paths when using Colab\n","TEMPLATE = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/templates'\n","STATIC = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/static'\n"," \n","#create flask app \n","app = Flask(__name__,\n","            template_folder=TEMPLATE,\n","            static_folder=STATIC)\n","\n","# run with ngrok when using Colab\n","run_with_ngrok(app)\n","\n","# model paths when using Colab\n","seq2seq_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/seq2seq'\n","intents_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/intents'\n","models_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/models'\n","\n","class chatbot:\n","    def __init__(self):\n","        self.max_vocab_size = 50000\n","        self.max_seq_len = 30\n","        self.embedding_dim = 100\n","        self.hidden_state_dim = 100\n","        self.epochs = 80\n","        self.batch_size = 128\n","        self.learning_rate = 1e-4\n","        self.dropout = 0.3\n","        self.data_path = r'G:\\My Drive\\chatbot\\twcs.csv'\n","        self.outpath = seq2seq_path\n","        self.version = 'v1'\n","        self.mode = 'inference'\n","        self.num_train_records = 50000\n","        self.load_model_from = os.path.join(seq2seq_path, 's2s_model_v1_.h5')\n","        self.vocabulary_path = os.path.join(seq2seq_path, 'vocabulary.pkl')\n","        self.reverse_vocabulary_path = os.path.join(seq2seq_path, 'reverse_vocabulary.pkl')\n","        self.count_vectorizer_path = os.path.join(seq2seq_path, 'count_vectorizer.pkl')\n","        self.t_path = os.path.join(intents_path, 'tokenizer.pickle')\n","        self.UNK = 0\n","        self.PAD = 1\n","        self.START = 2\n","\n","        # intent model variables\n","        self.intent_load_model_from = os.path.join(intents_path, 'pretrained_embeddings.h5')\n","        #self.intent_load_model_from = os.path.join(models_path, 'best_model_pretrained.h5')\n","        self.intent_load_intents_from = os.path.join(intents_path, 'intents_job_intents.json')\n","        self.intent_load_classes = os.path.join(intents_path, 'intents_classes.pkl')\n","        self.intent_load_words = os.path.join(intents_path, 'intents_words.pkl')\n","\n","    def process_data(self, path):\n","        data = pd.read_csv(path)\n","        if self.mode =='train':\n","            data = pd.read_csv(path)\n","            data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n","            tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n","            tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n","            return tweets_in_out[:self.num_train_records]\n","        elif self.mode == 'inference':\n","            return data\n","\n","    def replace_anonymized_names(self, data):\n","\n","        def replace_name(match):\n","            cname = match.group(2).lower()\n","            if not cname.isnumeric():\n","                return match.group(1) + match.group(2)\n","            return '@__cname__'\n","\n","            re_pattern = re.compile('(@|Y@)([a-zA-Z0-9_]+)')\n","            if self.mode == 'train':\n","                in_text = data['text_x'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","                out_text = data['text_y'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","                return list(in_text.values), list(out_text.values)\n","            else:\n","                return list(map(lambda x: re_pattern.sub(replace_name, x), data))\n","\n","    def tokenize_text(self, in_text, out_text):\n","        count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n","        count_vectorizer.fit(in_text + out_text)\n","        self.analyzer = count_vectorizer.build_analyzer()\n","        self.vocabulary = {key_: value_ + 3 for key_, value_ in count_vectorizer.vocabulary_.items()}\n","        self.vocabulary['UNK'] = self.UNK\n","        self.vocabulary['PAD'] = self.PAD\n","        self.vocabulary['START'] = self.START\n","        self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n","        joblib.dump(self.vocabulary, self.outpath + 'vocabulary.pkl')\n","        joblib.dump(self.reverse_vocabulary, self.outpath + 'reverse_vocabulary.pkl')\n","        joblib.dump(count_vectorizer, self.outpath + 'count_vectorizer.pkl')\n","\n","    def words_to_indices(self, sent):\n","        word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [self.PAD] * self.max_seq_len\n","        word_indices = word_indices[:self.max_seq_len]\n","        return word_indices\n","\n","    def indices_to_words(self, indices):\n","        return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip()\n","\n","    def data_transform(self, in_text, out_text):\n","        X = [self.words_to_indices(s) for s in in_text]\n","        Y = [self.words_to_indices(s) for s in out_text]\n","        return np.array(X), np.array(Y)\n","\n","    def train_test_split_(self, X, Y):\n","        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n","        y_train = y_train[:, :, np.newaxis]\n","        y_test = y_test[:, :, np.newaxis]\n","        return X_train, X_test, y_train, y_test\n","\n","    def data_creation(self):\n","        data = self.process_data(self.data_path)\n","        in_text, out_text = self.replace_anonymized_names(data)\n","        test_sentences = []\n","        test_indexes = np.random.randint(1, self.num_train_records, 10)\n","        for ind in test_indexes:\n","            sent = in_text[ind]\n","            test_sentences.append(sent)\n","        self.tokenize_text(in_text, out_text)\n","        X, Y = self.data_transform(in_text, out_text)\n","        X_train, X_test, y_train, y_test = self.train_test_split_(X, Y)\n","        return X_train, X_test, y_train, y_test, test_sentences\n","\n","    def define_model(self):\n","\n","        # Embedding Layer\n","        embedding = Embedding(\n","            output_dim=self.embedding_dim,\n","            input_dim=self.max_vocab_size,\n","            input_length=self.max_seq_len,\n","            name='embedding',\n","        )\n","        # Encoder input\n","        encoder_input = Input(\n","            shape=(self.max_seq_len,),\n","            dtype='int32',\n","            name='encoder_input',\n","        )\n","        embedded_input = embedding(encoder_input)\n","\n","        encoder_rnn = LSTM(\n","            self.hidden_state_dim,\n","            name='encoder',\n","            dropout=self.dropout\n","        )\n","\n","        # Context is repeated to the max sequence length so that the same context\n","        # can be feed at each step of decoder\n","        context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n","\n","        # Decoder\n","        last_word_input = Input(\n","            shape=(self.max_seq_len,),\n","            dtype='int32',\n","            name='last_word_input',\n","        )\n","\n","        embedded_last_word = embedding(last_word_input)\n","        # Combines the context produced by the encoder and the last word uttered as inputs\n","        # to the decoder.\n","\n","        decoder_input = concatenate([embedded_last_word, context], axis=2)\n","\n","        # return_sequences causes LSTM to produce one output per timestep instead of one at the\n","        # end of the input, which is important for sequence producing models.\n","        decoder_rnn = LSTM(\n","            self.hidden_state_dim,\n","            name='decoder',\n","            return_sequences=True,\n","            dropout=self.dropout\n","        )\n","\n","        decoder_output = decoder_rnn(decoder_input)\n","\n","        #TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n","        next_word_dense = TimeDistributed(\n","            Dense(int(self.max_vocab_size / 20), activation='relu'),\n","            name='next_word_dense',\n","        )(decoder_output)\n","\n","        next_word = TimeDistributed(\n","            Dense(self.max_vocab_size, activation='softmax'),\n","            name='next_word_softmax'\n","        )(next_word_dense)\n","\n","        return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n","\n","    def create_model(self):\n","        _model_ = self.define_model()\n","        adam = Adam(learning_rate=self.learning_rate, clipvalue=5.0)\n","        _model_.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n","        return _model_\n","\n","    # Function to append the START indext to the response Y\n","    def include_start_token(self, Y):\n","        print(Y.shape)\n","        Y = Y.reshape((Y.shape[0], Y.shape[1]))\n","        Y = np.hstack((self.START * np.ones((Y.shape[0], 1)), Y[:, :-1]))\n","        # Y = Y[:,:,np.newaxis]\n","        return Y\n","\n","    def binarize_output_response(self, Y):\n","        return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n","                        for row in Y])\n","\n","    def respond_to_input(self, model, input_sent):\n","        input_y = self.include_start_token(self.PAD *np.ones((1, self.max_seq_len)))\n","        ids = np.array(self.words_to_indices(input_sent)).reshape((1, self.max_seq_len))\n","        for pos in range(self.max_seq_len - 1):\n","            pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n","            # pred = model.predict([ids, input_y])[0]\n","            input_y[:, pos + 1] = pred[pos]\n","        return self.indices_to_words(model.predict([ids, input_y]).argmax(axis=2)[0])\n","\n","    def train_model(self, model, X_train, X_test, y_train, y_test):\n","        input_y_train = self.include_start_token(y_train)\n","        print(input_y_train.shape)\n","        input_y_test = self.include_start_token(y_test)\n","        print(input_y_test.shape)\n","        early = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n","\n","        checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5', monitor='val_loss',\n","                                     verbose=1, save_best_only=True, mode='auto')\n","\n","        lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, mode='auto')\n","\n","        model.fit([X_train, input_y_train], y_train,\n","                   epochs=self.epochs,\n","                   batch_size=self.batch_size,\n","                   validation_data=([X_test, input_y_test], y_test),\n","                   callbacks=[early, checkpoint, lr_reduce],\n","                   shuffle=True)\n","\n","        return model\n","\n","    def generate_response(self, model, sentences):\n","        output_responses = []\n","        print(sentences)\n","        for sent in sentences:\n","            response = self.respond_to_input(model, sent)\n","            output_responses.append(response)\n","        out_df = pd.DataFrame()\n","        out_df['Tweet in'] = sentences\n","        out_df['Tweet out'] = output_responses\n","        return out_df\n","\n","    def convert_to_sequence(self, sentence):\n","        print(f'Sentence 2: {sentence}')\n","        print(f'Sentence list: {[sentence]}')\n","        sequence = self.tkizer.texts_to_sequences([sentence])\n","        \n","        print(f'Initial Tokenization: {sequence}')\n","        sequence = pad_sequences(sequence, maxlen=25)\n","        print\n","        #sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","        return sequence\n","\n","    # return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","    def word_embedding(self, sentence, intent_words, show_details=True):\n","        # tokenize the pattern\n","        # intent words = all words\n","        print(f'Sentence 1: {sentence}')\n","        sequence = self.convert_to_sequence(sentence)\n","        # bag of words - matrix of N words, vocabulary matrix\n","\n","        return(sequence)\n","\n","    def predict_class(self, sentence, model):\n","        # filter predictions below a threshold\n","        # sentence is usertext\n","        # intent words are all the words\n","        print(f'sentence: {sentence}')\n","        sequence = self.word_embedding(sentence, self.intent_words, show_details=False)\n","        print(f'final sequence: {sequence}')\n","        res = model.predict(np.array(sequence))[0]\n","        print(f'res: {res}')\n","        ERROR_THRESHOLD = 0.25\n","        results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n","        print(f'results: {results}')\n","        # sort by strength of probability\n","        results.sort(key=lambda x: x[1], reverse=True)\n","        return_list = []\n","        print(f'return_list: {return_list}')\n","        for r in results:\n","            return_list.append({'intent': self.intent_classes[r[0]], 'probability': str(r[1])})\n","        print(f'return_list: {return_list}')\n","        return return_list\n","\n","    def getResponse(self, ints, intents_json):\n","        tag = ints[0]['intent']\n","        list_of_intents = intents_json['intents']\n","        for i in list_of_intents:\n","            if(i['tag'] == tag):\n","                result = random.choice(i['responses'])\n","                break\n","            else:\n","                result = 'Please input a different message.'\n","        return result\n","\n","    def string_clean(self, response_orig):\n","\n","        def upper_repl(match):\n","            punctuated_inits = \\\n","                '-' + match.group(1).upper() + '.' \\\n","                     + match.group(2).upper() + '.'\n","            return punctuated_inits\n","\n","        response = response_orig\n","        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","        # remove '@__cname__'\n","        response = response.replace('@__cname__ ', '')\n","        \n","        # remove spaces before punctuation\n","        response = re.sub(r'\\s([,?.!\"](?:\\s|$))', r'\\1', response)\n","        # tokenize sentences\n","        sentences = sent_tokenizer.tokenize(response)\n","        # captialize senteces\n","        sentences = [sent.capitalize() for sent in sentences]\n","\n","        # add html formatting\n","        sentences = '</span><br><span>'.join(sentences)\n","        sentences += '</span>'\n","        # capitalize DM\n","        sentences = sentences.replace('dm', 'dm'.upper())\n","\n","        # replace '^' with '-'\n","        sentences = sentences.replace('^', '-')\n","        pattern = re.compile(r'- \\b([a-z])([a-z])\\b')\n","\n","        sentences = re.sub(pattern, upper_repl, sentences)\n","        return sentences\n","\n","    def main(self):\n","        if self.mode == 'train':\n","            X_train, X_test, y_train, y_test, test_sentences = self.data_creation()\n","            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","            print('Data Creation completed')\n","            model = self.create_model()\n","            print('Model creation completed')\n","            model = self.train_model(model, X_train, X_test, y_train, y_test)\n","            test_responses = self.generate_response(model, test_sentences)\n","            print(test_sentences)\n","            print(test_responses)\n","            pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv', index=False)\n","     \n","        elif self.mode == 'inference':\n","            #seq2seq model\n","            model = load_model(self.load_model_from)\n","            self.vocabulary = joblib.load(os.path.join(self.outpath, 'vocabulary.pkl'))\n","            self.reverse_vocabulary = joblib.load(os.path.join(self.outpath, 'reverse_vocabulary.pkl'))\n","            count_vectorizer = joblib.load(os.path.join(self.outpath, 'count_vectorizer.pkl'))\n","            self.analyzer = count_vectorizer.build_analyzer()\n","\n","            #load intent model\n","            intent_model = load_model(self.intent_load_model_from)\n","            self.intent_intents = json.loads(open(self.intent_load_intents_from, encoding='cp1252').read())\n","            self.intent_words = pickle.load(open(self.intent_load_words,'rb'))\n","            self.intent_classes = pickle.load(open(self.intent_load_classes,'rb'))\n","            self.tkizer = pickle.load(open(self.t_path,'rb'))\n","\n","            while True:\n","                try:\n","                    userText = request.args.get('msg')\n","                    ints = self.predict_class(userText, intent_model)\n","                    intent_response = self.getResponse(ints, self.intent_intents)\n","                    if (intent_response != 'help'):\n","                        return str(intent_response)\n","                    elif (intent_response == 'help'):\n","                        response = self.respond_to_input(model, userText)\n","                        response = self.string_clean(response)\n","                        return str(response)\n","\n","                except(KeyboardInterrupt, EOFError, SystemExit):\n","                    break\n","\n","        \n","\n","@app.route(\"/\")\n","def home():\n","    return render_template(\"index.html\")\n","\n","@app.route(\"/get\")\n","def get_bot_response():\n","    obj = chatbot()\n","    obj.mode = 'inference'\n","    response = obj.main()\n","    return response\n","\n","app.run()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://0ba8-35-230-5-149.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:45:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 08:45:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 08:45:51] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 08:45:51] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Oct/2021 08:45:51] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]},{"output_type":"stream","name":"stdout","text":["sentence: Can you hear me?\n","Sentence 1: Can you hear me?\n","Sentence 2: Can you hear me?\n","Sentence list: ['Can you hear me?']\n","Initial Tokenization: [[15, 2, 46, 14]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  2 46\n","  14]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:46:27] \"\u001b[37mGET /get?msg=Can%20you%20hear%20me%3F HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [5.8786300e-06 1.8493751e-04 6.0655188e-05 9.9970323e-01 1.8569312e-05\n"," 1.2375410e-05 2.7233028e-07 5.2441524e-06 1.6324925e-10 8.7719991e-06\n"," 4.7346390e-08]\n","results: [[3, 0.9997032]]\n","return_list: []\n","return_list: [{'intent': 'greeting', 'probability': '0.9997032'}]\n","sentence: How are you today?\n","Sentence 1: How are you today?\n","Sentence 2: How are you today?\n","Sentence list: ['How are you today?']\n","Initial Tokenization: [[6, 9, 2, 30]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  9  2\n","  30]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:48:56] \"\u001b[37mGET /get?msg=How%20are%20you%20today%3F HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [1.7366793e-04 9.9961638e-01 1.3951861e-09 1.6407335e-04 3.5676287e-05\n"," 6.1568512e-06 3.9077368e-06 4.8242789e-08 7.1149735e-08 4.5133234e-08\n"," 5.0723970e-11]\n","results: [[1, 0.9996164]]\n","return_list: []\n","return_list: [{'intent': 'feeling', 'probability': '0.9996164'}]\n","sentence: Can I see your manager?\n","Sentence 1: Can I see your manager?\n","Sentence 2: Can I see your manager?\n","Sentence list: ['Can I see your manager?']\n","Initial Tokenization: [[15, 1, 28, 7, 37]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  1 28  7\n","  37]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:49:20] \"\u001b[37mGET /get?msg=Can%20I%20see%20your%20manager%3F HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [2.5795841e-06 3.0200936e-06 2.0103737e-06 7.9161691e-07 9.9939024e-01\n"," 1.8997394e-05 4.6778537e-04 1.0425396e-04 1.0243381e-05 4.0752074e-10\n"," 1.0287041e-11]\n","results: [[4, 0.99939024]]\n","return_list: []\n","return_list: [{'intent': 'manager', 'probability': '0.99939024'}]\n","sentence: I have a product I need to return.\n","Sentence 1: I have a product I need to return.\n","Sentence 2: I have a product I need to return.\n","Sentence list: ['I have a product I need to return.']\n","Initial Tokenization: [[1, 33, 8, 11, 1, 12, 3, 16]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 33  8 11  1 12  3\n","  16]]\n","res: [4.9840249e-08 2.4297366e-09 5.9999589e-11 3.6729320e-14 1.7246585e-06\n"," 2.1154002e-10 4.1099684e-06 2.9968971e-06 9.9999106e-01 1.1008855e-13\n"," 1.3608999e-15]\n","results: [[8, 0.99999106]]\n","return_list: []\n","return_list: [{'intent': 'return_product', 'probability': '0.99999106'}]\n","(1, 30)\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f80d70d2dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:49:43] \"\u001b[37mGET /get?msg=I%20have%20a%20product%20I%20need%20to%20return. HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["sentence: I love you.\n","Sentence 1: I love you.\n","Sentence 2: I love you.\n","Sentence list: ['I love you.']\n","Initial Tokenization: [[1, 2]]\n","final sequence: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:50:04] \"\u001b[37mGET /get?msg=I%20love%20you. HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [2.2076161e-04 3.0677129e-05 1.1597060e-01 7.2880690e-03 3.7702383e-04\n"," 2.3960910e-04 2.9214323e-04 8.7182575e-01 5.4418302e-05 3.2657923e-03\n"," 4.3517910e-04]\n","results: [[7, 0.87182575]]\n","return_list: []\n","return_list: [{'intent': 'profane', 'probability': '0.87182575'}]\n","sentence: You are my favorite chatbot.\n","Sentence 1: You are my favorite chatbot.\n","Sentence 2: You are my favorite chatbot.\n","Sentence list: ['You are my favorite chatbot.']\n","Initial Tokenization: [[2, 9, 4]]\n","final sequence: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 9 4]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:50:25] \"\u001b[37mGET /get?msg=You%20are%20my%20favorite%20chatbot. HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [7.2469759e-01 1.9538207e-02 7.4681607e-03 3.6153022e-02 4.1928203e-03\n"," 1.4793736e-01 1.9197045e-02 1.2964440e-02 1.9815096e-03 2.5437040e-02\n"," 4.3273129e-04]\n","results: [[0, 0.7246976]]\n","return_list: []\n","return_list: [{'intent': 'compliment', 'probability': '0.7246976'}]\n","sentence: I want my money back.\n","Sentence 1: I want my money back.\n","Sentence 2: I want my money back.\n","Sentence list: ['I want my money back.']\n","Initial Tokenization: [[1, 5, 4, 22, 23]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  5  4 22\n","  23]]\n","res: [1.7532598e-08 1.1227128e-09 2.7629060e-10 2.8175816e-14 1.7025408e-06\n"," 5.6105932e-11 1.7277240e-05 1.4098287e-05 9.9996686e-01 1.7964113e-13\n"," 2.5577263e-15]\n","results: [[8, 0.99996686]]\n","return_list: []\n","return_list: [{'intent': 'return_product', 'probability': '0.99996686'}]\n","(1, 30)\n","WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f80d558b710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:52:30] \"\u001b[37mGET /get?msg=I%20want%20my%20money%20back. HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["sentence: Thank you so much.\n","Sentence 1: Thank you so much.\n","Sentence 2: Thank you so much.\n","Sentence list: ['Thank you so much.']\n","Initial Tokenization: [[59, 2]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59\n","   2]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:53:10] \"\u001b[37mGET /get?msg=Thank%20you%20so%20much. HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [1.43254976e-04 1.27949625e-06 4.44678211e-04 4.22557496e-04\n"," 7.90063268e-08 6.80175290e-05 1.08860945e-07 1.55341040e-05\n"," 1.44271439e-08 9.98807669e-01 9.68179083e-05]\n","results: [[9, 0.99880767]]\n","return_list: []\n","return_list: [{'intent': 'thanks', 'probability': '0.99880767'}]\n","sentence: What is your name?\n","Sentence 1: What is your name?\n","Sentence 2: What is your name?\n","Sentence list: ['What is your name?']\n","Initial Tokenization: [[60, 10, 7, 35]]\n","final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 60 10  7\n","  35]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:53:29] \"\u001b[37mGET /get?msg=What%20is%20your%20name%3F HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [4.9788203e-05 1.1314946e-04 6.1708141e-07 6.2602725e-05 2.1609239e-04\n"," 9.9955720e-01 3.0999828e-08 3.2864229e-08 1.5889064e-07 1.6996367e-07\n"," 3.9204706e-09]\n","results: [[5, 0.9995572]]\n","return_list: []\n","return_list: [{'intent': 'name', 'probability': '0.9995572'}]\n","sentence: Are you a real person?\n","Sentence 1: Are you a real person?\n","Sentence 2: Are you a real person?\n","Sentence list: ['Are you a real person?']\n","Initial Tokenization: [[9, 2, 8]]\n","final sequence: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 2 8]]\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Oct/2021 08:53:58] \"\u001b[37mGET /get?msg=Are%20you%20a%20real%20person%3F HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["res: [4.3347496e-01 2.0399368e-01 1.7114346e-04 9.6862704e-02 6.0622818e-03\n"," 2.5084388e-01 5.8525923e-04 2.5605394e-03 3.6692955e-03 1.7267833e-03\n"," 4.9442577e-05]\n","results: [[0, 0.43347496], [5, 0.25084388]]\n","return_list: []\n","return_list: [{'intent': 'compliment', 'probability': '0.43347496'}, {'intent': 'name', 'probability': '0.25084388'}]\n"]}]},{"cell_type":"markdown","metadata":{"id":"-FSek_u97lkg"},"source":[""]}]}