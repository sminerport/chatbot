{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"flask-chatterbot-simple.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1kCQkP7eEvxrpkp79sfdkAVyIKw9jSugC","authorship_tag":"ABX9TyObT7YyRTxcu3A15Ph8Gouj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AC9_kW-ZlS6W","collapsed":true,"executionInfo":{"status":"ok","timestamp":1632684839118,"user_tz":360,"elapsed":3080,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"53f4261d-8f77-4be7-9f63-9499eff9909a"},"source":["!pip install flask-ngrok"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","metadata":{"id":"kVY_OFslacFr"},"source":["!cp '/content/drive/MyDrive/Colab Notebooks/chatbot-portfolio/data/s2s_model_v1_.h5' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5euw18AIcGED"},"source":["!cp '/content/drive/MyDrive/Colab Notebooks/chatbot-portfolio/data/count_vectorizer.pkl' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyGwGzo5cLKh"},"source":["!cp '/content/drive/MyDrive/Colab Notebooks/chatbot-portfolio/data/reverse_vocabulary.pkl' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWqIvqrDcR8c"},"source":["!cp '/content/drive/MyDrive/Colab Notebooks/chatbot-portfolio/data/vocabulary.pkl' ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4hEVDR2nqWw"},"source":["TEMPLATE = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/templates'\n","STATIC = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/static'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQv6NX-YlnAg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c330efd-cbfc-4411-8374-5f661ae9aded"},"source":["from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request\n","import re\n","import os\n","from time import time\n","\n","import numpy as np\n","import pandas as pd\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from keras.layers import Dense, Input, LSTM, Embedding, RepeatVector, concatenate, TimeDistributed\n","from keras.models import Model\n","from keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","from keras.utils import np_utils\n","from nltk.tokenize import casual_tokenize\n","import joblib\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","#english_bot = ChatBot(\"Chatterbot\", storage_adapter=\"chatterbot.storage.SQLStorageAdapter\")\n","#trainer = ChatterBotCorpusTrainer(english_bot)\n","#trainer.train(\"chatterbot.corpus.english\")\n"," \n","app = Flask(__name__,\n","            template_folder=TEMPLATE,\n","            static_folder=STATIC)\n","run_with_ngrok(app)\n","\n","# model path\n","path = '/content/drive/MyDrive/Colab Notebooks/chatbot-portfolio/data'\n","\n","class chatbot:\n","     def __init__(self):\n","          self.max_vocab_size = 50000\n","          self.max_seq_len = 30\n","          self.embedding_dim = 100\n","          self.hidden_state_dim = 100\n","          self.epochs = 80\n","          self.batch_size = 128\n","          self.learning_rate = 1e-4\n","          self.dropout = 0.3\n","          self.data_path = r'G:\\My Drive\\chatbot\\twcs.csv'\n","          self.outpath = r'/content'                         \n","          self.version = 'v1'\n","          self.mode = 'inference'\n","          self.num_train_records = 50000\n","          self.load_model_from = os.path.join(path, 's2s_model_v1_.h5')\n","          self.vocabulary_path = os.path.join(path, 'vocabulary.pkl')\n","          self.reverse_vocabulary_path = os.path.join(path, 'reverse_vocabulary.pkl')\n","          self.count_vectorizer_path = os.path.join(path, 'count_vectorizer.pkl')\n","          self.UNK = 0\n","          self.PAD = 1\n","          self.START = 2\n","\n","     def process_data(self, path):\n","          data = pd.read_csv(path)\n","          if self.mode =='train':\n","               data = pd.read_csv(path)\n","               data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n","               tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n","               tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n","               return tweets_in_out[:self.num_train_records]\n","          elif self.mode == 'inference':\n","               return data\n","     \n","     def replace_anonymized_names(self, data):\n","          \n","          def replace_name(match):\n","               cname = match.group(2).lower()\n","               if not cname.isnumeric():\n","                    return match.group(1) + match.group(2)\n","               return '@__cname__'\n","     \n","          re_pattern = re.compile('(@|Y@)([a-zA-Z0-9_]+)')\n","          if self.mode == 'train':\n","               in_text = data['text_x'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","               out_text = data['text_y'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","               return list(in_text.values), list(out_text.values)\n","          else:\n","               return list(map(lambda x: re_pattern.sub(replace_name, x), data))\n","     \n","     def tokenize_text(self, in_text, out_text):\n","          count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n","          count_vectorizer.fit(in_text + out_text)\n","          self.analyzer = count_vectorizer.build_analyzer()\n","          self.vocabulary = {key_: value_ + 3 for key_, value_ in count_vectorizer.vocabulary_.items()}\n","          self.vocabulary['UNK'] = self.UNK\n","          self.vocabulary['PAD'] = self.PAD\n","          self.vocabulary['START'] = self.START\n","          self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n","          joblib.dump(self.vocabulary, self.outpath + 'vocabulary.pkl')\n","          joblib.dump(self.reverse_vocabulary, self.outpath + 'reverse_vocabulary.pkl')\n","          joblib.dump(count_vectorizer, self.outpath + 'count_vectorizer.pkl')\n","     \n","     def words_to_indices(self, sent):\n","          word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [self.PAD] * self.max_seq_len\n","          word_indices = word_indices[:self.max_seq_len]\n","          return word_indices\n","     \n","     def indices_to_words(self, indices):\n","          return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip()\n","     \n","     def data_transform(self, in_text, out_text):\n","          X = [self.words_to_indices(s) for s in in_text]\n","          Y = [self.words_to_indices(s) for s in out_text]\n","          return np.array(X), np.array(Y)\n","     \n","     def train_test_split_(self, X, Y):\n","          X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n","          y_train = y_train[:, :, np.newaxis]\n","          y_test = y_test[:, :, np.newaxis]\n","          return X_train, X_test, y_train, y_test\n","     \n","     def data_creation(self):\n","          data = self.process_data(self.data_path)\n","          in_text, out_text = self.replace_anonymized_names(data)\n","          test_sentences = []\n","          test_indexes = np.random.randint(1, self.num_train_records, 10)\n","          for ind in test_indexes:\n","               sent = in_text[ind]\n","               test_sentences.append(sent)\n","          self.tokenize_text(in_text, out_text)\n","          X, Y = self.data_transform(in_text, out_text)\n","          X_train, X_test, y_train, y_test = self.train_test_split_(X, Y)\n","          return X_train, X_test, y_train, y_test, test_sentences\n","     \n","     def define_model(self):\n","     \n","          # Embedding Layer\n","          embedding = Embedding(\n","               output_dim=self.embedding_dim,\n","               input_dim=self.max_vocab_size,\n","               input_length=self.max_seq_len,\n","               name='embedding',\n","          )\n","          # Encoder input\n","          encoder_input = Input(\n","               shape=(self.max_seq_len,),\n","               dtype='int32',\n","               name='encoder_input',\n","          )\n","          embedded_input = embedding(encoder_input)\n","          \n","          encoder_rnn = LSTM(\n","               self.hidden_state_dim,\n","               name='encoder',\n","               dropout=self.dropout\n","          )\n","     \n","          # Context is repeated to the max sequence length so that the same context\n","          # can be feed at each step of decoder\n","          context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n","     \n","          # Decoder\n","          last_word_input = Input(\n","               shape=(self.max_seq_len,),\n","               dtype='int32',\n","               name='last_word_input',\n","          )\n","     \n","          embedded_last_word = embedding(last_word_input)\n","          # Combines the context produced by the encoder and the last word uttered as inputs\n","          # to the decoder.\n","          \n","          decoder_input = concatenate([embedded_last_word, context], axis=2)\n","     \n","          # return_sequences causes LSTM to produce one output per timestep instead of one at the\n","          # end of the input, which is important for sequence producing models.\n","          decoder_rnn = LSTM(\n","               self.hidden_state_dim,\n","               name='decoder',\n","               return_sequences=True,\n","               dropout=self.dropout\n","          )\n","     \n","          decoder_output = decoder_rnn(decoder_input)\n","     \n","          #TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n","          next_word_dense = TimeDistributed(\n","               Dense(int(self.max_vocab_size / 20), activation='relu'),\n","               name='next_word_dense',\n","          )(decoder_output)\n","     \n","          next_word = TimeDistributed(\n","               Dense(self.max_vocab_size, activation='softmax'),\n","               name='next_word_softmax'\n","          )(next_word_dense)\n","     \n","          return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n","     \n","     def create_model(self):\n","          _model_ = self.define_model()\n","          adam = Adam(learning_rate=self.learning_rate, clipvalue=5.0)\n","          _model_.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n","          return _model_\n","     \n","     # Function to append the START indext to the response Y\n","     def include_start_token(self, Y):\n","          print(Y.shape)\n","          Y = Y.reshape((Y.shape[0], Y.shape[1]))\n","          Y = np.hstack((self.START * np.ones((Y.shape[0], 1)), Y[:, :-1]))\n","          # Y = Y[:,:,np.newaxis]\n","          return Y\n","     \n","     def binarize_output_response(self, Y):\n","          return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n","                          for row in Y])\n","     \n","     \n","     def respond_to_input(self, model, input_sent):\n","          input_y = self.include_start_token(self.PAD *np.ones((1, self.max_seq_len)))\n","          ids = np.array(self.words_to_indices(input_sent)).reshape((1, self.max_seq_len))\n","          for pos in range(self.max_seq_len - 1):\n","               pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n","               # pred = model.predict([ids, input_y])[0]\n","               input_y[:, pos + 1] = pred[pos]\n","          return self.indices_to_words(model.predict([ids, input_y]).argmax(axis=2)[0])\n","     \n","     def train_model(self, model, X_train, X_test, y_train, y_test):\n","          input_y_train = self.include_start_token(y_train)\n","          print(input_y_train.shape)\n","          input_y_test = self.include_start_token(y_test)\n","          print(input_y_test.shape)\n","          early = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n","     \n","          checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5', monitor='val_loss',\n","                                       verbose=1, save_best_only=True, mode='auto')\n","     \n","          lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, mode='auto')\n","     \n","          model.fit([X_train, input_y_train], y_train,\n","                    epochs=self.epochs,\n","                    batch_size=self.batch_size,\n","                    validation_data=([X_test, input_y_test], y_test),\n","                    callbacks=[early, checkpoint, lr_reduce],\n","                    shuffle=True)\n","          return model\n","     \n","     def generate_response(self, model, sentences):\n","          output_responses = []\n","          print(sentences)\n","          for sent in sentences:\n","               response = self.respond_to_input(model, sent)\n","               output_responses.append(response)\n","          out_df = pd.DataFrame()\n","          out_df['Tweet in'] = sentences\n","          out_df['Tweet out'] = output_responses\n","          return out_df\n","     \n","     def main(self):\n","          if self.mode == 'train':\n","               X_train, X_test, y_train, y_test, test_sentences = self.data_creation()\n","               print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","               print('Data Creation completed')\n","               model = self.create_model()\n","               print('Model creation completed')\n","               model = self.train_model(model, X_train, X_test, y_train, y_test)\n","               test_responses = self.generate_response(model, test_sentences)\n","               print(test_sentences)\n","               print(test_responses)\n","               pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv', index=False)\n","     \n","          elif self.mode == 'inference':\n","               model = load_model(self.load_model_from)\n","               self.vocabulary = joblib.load(self.outpath + '/vocabulary.pkl')\n","               self.reverse_vocabulary = joblib.load(self.outpath + '/reverse_vocabulary.pkl')\n","               # nalyzer_file = open(self.analyzer_path,'rb')\n","               count_vectorizer = joblib.load(self.outpath + '/count_vectorizer.pkl')\n","               self.analyzer = count_vectorizer.build_analyzer()\n","               #data = self.process_data(self.data_path)\n","               #col = data.columns.tolist()[4]\n","               #test_sentences = list(data[col].values)\n","               while True:\n","                    try:\n","                         userText = request.args.get('msg')\n","                         #return str(english_bot.get_response(userText))\n","                         response = self.respond_to_input(model, userText)\n","                         return str(response)\n","                         #print(f'Chatbot: {response}')\n","                    except(KeyboardInterrupt, EOFError, SystemExit):\n","                         break\n","               #test_sentences = self.replace_anonymized_names(test_sentences[0:10])\n","               #responses = self.generate_response(model, test_sentences[0:10])\n","               #print(responses)\n","               #responses.to_csv(self.outpath + 'responses_' + str(self.version) + '_.csv', index=False)\n","\n","#start_time = time()\n","#obj = chatbot()\n","#obj.mode = 'inference'\n","#obj.main()\n","#end_time = time()\n","#print(f'Processing finished, time taken is {end_time - start_time}')\n"," \n","@app.route(\"/\")\n","def home():\n","    return render_template(\"index.html\")\n"," \n","@app.route(\"/get\")\n","def get_bot_response():\n","    obj = chatbot()\n","    obj.mode = 'inference'\n","    response = obj.main()\n","    return response\n","    \n","    #userText = request.args.get('msg')\n","    #return str(english_bot.get_response(userText))\n","\n","app.run()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://78c1-34-105-124-237.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:26:19] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:19] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:20] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:21] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [26/Sep/2021 23:26:49] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:49] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [26/Sep/2021 23:26:50] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:27:12] \"\u001b[37mGET /get?msg=I%20have%20a%20broken%20chair HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:28:21] \"\u001b[37mGET /get?msg=hdjdhdgfb%40gmaill.com HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:28:54] \"\u001b[37mGET /get?msg=I%20want%20to%20return%20my%20item HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:29:31] \"\u001b[37mGET /get?msg=it%20worked%20yesterday%2C%20now%20it%20doesnt%20work HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:29:56] \"\u001b[37mGET /get?msg=it%20is%20blue%20and%20I%20want%20red HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:30:23] \"\u001b[37mGET /get?msg=%3B%3BLAKSDIEIH%20JSJKjdhu%20alksdj%20%3B%3Bsldythr%20kshalr HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:31:11] \"\u001b[37mGET /get?msg=are%20you%20a%20female HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:31:47] \"\u001b[37mGET /get?msg=I%20want%20to%20eat%20dinner%2C%20do%20you%20want%20to%20eat HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:32:13] \"\u001b[37mGET /get?msg=I%20live%20in%20California HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:32:57] \"\u001b[37mGET /get?msg=I%20am%20going%20to%20go%20bye HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:33:13] \"\u001b[37mGET /get?msg=bye HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:33:31] \"\u001b[37mGET /get?msg=drop%20dead HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:34:29] \"\u001b[37mGET /get?msg=My%20HW%20is%20late%20is%20that%20OK HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:35:42] \"\u001b[37mGET /get?msg=I%20would%20like%20to%20order%202%20pipes HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [26/Sep/2021 23:36:21] \"\u001b[37mGET /get?msg=Do%20you%20want%20to%20marry%20me HTTP/1.1\u001b[0m\" 200 -\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEeqMocBUERY","executionInfo":{"status":"ok","timestamp":1632780737260,"user_tz":360,"elapsed":570,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"12e9b508-1dfa-4688-c698-1723900ed1a8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Q_FzPReXOHD","executionInfo":{"status":"ok","timestamp":1632781583784,"user_tz":360,"elapsed":4648,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"78ce090d-dd07-47f3-903a-42f98ecc6fa6"},"source":["! init ."],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/.git/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3aDJZjBXapR","executionInfo":{"status":"ok","timestamp":1632781708983,"user_tz":360,"elapsed":166,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"de861bbe-d5f5-4f76-ae58-2794acc77cac"},"source":["!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEB_KD55X6UO","executionInfo":{"status":"ok","timestamp":1632781728962,"user_tz":360,"elapsed":226,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"a70bb5bb-a734-4bf1-e56c-abe452267eb0"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCNJGIDiX_J2","executionInfo":{"status":"ok","timestamp":1632781820434,"user_tz":360,"elapsed":5261,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"7d13ef0b-1345-4430-bf59-02e690222da7"},"source":["!git init '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple'\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized empty Git repository in /content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/.git/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyT73jGDYUO5","executionInfo":{"status":"ok","timestamp":1632781846655,"user_tz":360,"elapsed":3803,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"f38663d0-de2d-498b-ad32-1a3240a85c4b"},"source":["%cd '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple'"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QOADErpYa8p","executionInfo":{"status":"ok","timestamp":1632781874417,"user_tz":360,"elapsed":206,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"53b33795-6fd3-4ec3-9376-54a03cb8cc0e"},"source":["%ls -a"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["flask-chatterbot-simple.ipynb  \u001b[0m\u001b[01;34m.git\u001b[0m/  \u001b[01;34m.ipynb_checkpoints\u001b[0m/  \u001b[01;34mstatic\u001b[0m/  \u001b[01;34mtemplates\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfSSZLlpYh5Z","executionInfo":{"status":"ok","timestamp":1632781909444,"user_tz":360,"elapsed":151,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"23145593-c1ea-4297-b303-ce4e3eca437b"},"source":["!git status"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch master\n","\n","No commits yet\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mflask-chatterbot-simple.ipynb\u001b[m\n","\t\u001b[31mstatic/\u001b[m\n","\t\u001b[31mtemplates/\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","metadata":{"id":"4mip-4Q5Yqjr"},"source":["!git add static/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZlOIKoAZGEi"},"source":[""],"execution_count":null,"outputs":[]}]}