{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"flask-chatterbot-simple.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1kCQkP7eEvxrpkp79sfdkAVyIKw9jSugC","authorship_tag":"ABX9TyNUjQWXNP9DAVcbAJTOb0T3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AC9_kW-ZlS6W","collapsed":true,"executionInfo":{"status":"ok","timestamp":1632841795842,"user_tz":360,"elapsed":4636,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"10c449f5-c7e1-422a-a603-8573e890198d"},"source":["!pip install flask-ngrok"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","metadata":{"id":"WQv6NX-YlnAg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632850486490,"user_tz":360,"elapsed":46338,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"6cc3bf57-81cb-4207-ef1b-5ed52cd1cfe1"},"source":["from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request\n","import re\n","import os\n","from time import time\n","\n","import numpy as np\n","import pandas as pd\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from keras.layers import Dense, Input, LSTM, Embedding, RepeatVector, concatenate, TimeDistributed\n","from keras.models import Model\n","from keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","from keras.utils import np_utils\n","from nltk.tokenize import casual_tokenize\n","import joblib\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","import pickle\n","import numpy as np\n","from keras.models import load_model\n","\n","import json\n","import random\n","\n","# fold paths when using Colab\n","TEMPLATE = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/templates'\n","STATIC = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/static'\n"," \n","#create flask app \n","app = Flask(__name__,\n","            template_folder=TEMPLATE,\n","            static_folder=STATIC)\n","\n","# run with ngrok when using Colab\n","run_with_ngrok(app)\n","\n","# model paths when using Colab\n","seq2seq_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/seq2seq'\n","intents_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/intents'\n","\n","class chatbot:\n","    def __init__(self):\n","        self.max_vocab_size = 50000\n","        self.max_seq_len = 30\n","        self.embedding_dim = 100\n","        self.hidden_state_dim = 100\n","        self.epochs = 80\n","        self.batch_size = 128\n","        self.learning_rate = 1e-4\n","        self.dropout = 0.3\n","        self.data_path = r'G:\\My Drive\\chatbot\\twcs.csv'\n","        self.outpath = seq2seq_path\n","        self.version = 'v1'\n","        self.mode = 'inference'\n","        self.num_train_records = 50000\n","        self.load_model_from = os.path.join(seq2seq_path, 's2s_model_v1_.h5')\n","        self.vocabulary_path = os.path.join(seq2seq_path, 'vocabulary.pkl')\n","        self.reverse_vocabulary_path = os.path.join(seq2seq_path, 'reverse_vocabulary.pkl')\n","        self.count_vectorizer_path = os.path.join(seq2seq_path, 'count_vectorizer.pkl')\n","        self.UNK = 0\n","        self.PAD = 1\n","        self.START = 2\n","\n","        # inference model variables\n","        self.inference_load_model_from = os.path.join(intents_path, 'intents_chatbot_model.h5')\n","        self.inference_load_intents_from = os.path.join(intents_path, 'intents_job_intents.json')\n","        self.inference_load_classes = os.path.join(intents_path, 'intents_classes.pkl')\n","        self.inference_load_words = os.path.join(intents_path, 'intents_words.pkl')\n","\n","    def process_data(self, path):\n","        data = pd.read_csv(path)\n","        if self.mode =='train':\n","            data = pd.read_csv(path)\n","            data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n","            tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n","            tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n","            return tweets_in_out[:self.num_train_records]\n","        elif self.mode == 'inference':\n","            return data\n","\n","    def replace_anonymized_names(self, data):\n","\n","        def replace_name(match):\n","            cname = match.group(2).lower()\n","            if not cname.isnumeric():\n","                return match.group(1) + match.group(2)\n","            return '@__cname__'\n","\n","            re_pattern = re.compile('(@|Y@)([a-zA-Z0-9_]+)')\n","            if self.mode == 'train':\n","                in_text = data['text_x'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","                out_text = data['text_y'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n","                return list(in_text.values), list(out_text.values)\n","            else:\n","                return list(map(lambda x: re_pattern.sub(replace_name, x), data))\n","\n","    def tokenize_text(self, in_text, out_text):\n","        count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n","        count_vectorizer.fit(in_text + out_text)\n","        self.analyzer = count_vectorizer.build_analyzer()\n","        self.vocabulary = {key_: value_ + 3 for key_, value_ in count_vectorizer.vocabulary_.items()}\n","        self.vocabulary['UNK'] = self.UNK\n","        self.vocabulary['PAD'] = self.PAD\n","        self.vocabulary['START'] = self.START\n","        self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n","        joblib.dump(self.vocabulary, self.outpath + 'vocabulary.pkl')\n","        joblib.dump(self.reverse_vocabulary, self.outpath + 'reverse_vocabulary.pkl')\n","        joblib.dump(count_vectorizer, self.outpath + 'count_vectorizer.pkl')\n","\n","    def words_to_indices(self, sent):\n","        word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [self.PAD] * self.max_seq_len\n","        word_indices = word_indices[:self.max_seq_len]\n","        return word_indices\n","\n","    def indices_to_words(self, indices):\n","        return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip()\n","\n","    def data_transform(self, in_text, out_text):\n","        X = [self.words_to_indices(s) for s in in_text]\n","        Y = [self.words_to_indices(s) for s in out_text]\n","        return np.array(X), np.array(Y)\n","\n","    def train_test_split_(self, X, Y):\n","        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n","        y_train = y_train[:, :, np.newaxis]\n","        y_test = y_test[:, :, np.newaxis]\n","        return X_train, X_test, y_train, y_test\n","\n","    def data_creation(self):\n","        data = self.process_data(self.data_path)\n","        in_text, out_text = self.replace_anonymized_names(data)\n","        test_sentences = []\n","        test_indexes = np.random.randint(1, self.num_train_records, 10)\n","        for ind in test_indexes:\n","            sent = in_text[ind]\n","            test_sentences.append(sent)\n","        self.tokenize_text(in_text, out_text)\n","        X, Y = self.data_transform(in_text, out_text)\n","        X_train, X_test, y_train, y_test = self.train_test_split_(X, Y)\n","        return X_train, X_test, y_train, y_test, test_sentences\n","\n","    def define_model(self):\n","\n","        # Embedding Layer\n","        embedding = Embedding(\n","            output_dim=self.embedding_dim,\n","            input_dim=self.max_vocab_size,\n","            input_length=self.max_seq_len,\n","            name='embedding',\n","        )\n","        # Encoder input\n","        encoder_input = Input(\n","            shape=(self.max_seq_len,),\n","            dtype='int32',\n","            name='encoder_input',\n","        )\n","        embedded_input = embedding(encoder_input)\n","\n","        encoder_rnn = LSTM(\n","            self.hidden_state_dim,\n","            name='encoder',\n","            dropout=self.dropout\n","        )\n","\n","        # Context is repeated to the max sequence length so that the same context\n","        # can be feed at each step of decoder\n","        context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n","\n","        # Decoder\n","        last_word_input = Input(\n","            shape=(self.max_seq_len,),\n","            dtype='int32',\n","            name='last_word_input',\n","        )\n","\n","        embedded_last_word = embedding(last_word_input)\n","        # Combines the context produced by the encoder and the last word uttered as inputs\n","        # to the decoder.\n","\n","        decoder_input = concatenate([embedded_last_word, context], axis=2)\n","\n","        # return_sequences causes LSTM to produce one output per timestep instead of one at the\n","        # end of the input, which is important for sequence producing models.\n","        decoder_rnn = LSTM(\n","            self.hidden_state_dim,\n","            name='decoder',\n","            return_sequences=True,\n","            dropout=self.dropout\n","        )\n","\n","        decoder_output = decoder_rnn(decoder_input)\n","\n","        #TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n","        next_word_dense = TimeDistributed(\n","            Dense(int(self.max_vocab_size / 20), activation='relu'),\n","            name='next_word_dense',\n","        )(decoder_output)\n","\n","        next_word = TimeDistributed(\n","            Dense(self.max_vocab_size, activation='softmax'),\n","            name='next_word_softmax'\n","        )(next_word_dense)\n","\n","        return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n","\n","    def create_model(self):\n","        _model_ = self.define_model()\n","        adam = Adam(learning_rate=self.learning_rate, clipvalue=5.0)\n","        _model_.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n","        return _model_\n","\n","    # Function to append the START indext to the response Y\n","    def include_start_token(self, Y):\n","        print(Y.shape)\n","        Y = Y.reshape((Y.shape[0], Y.shape[1]))\n","        Y = np.hstack((self.START * np.ones((Y.shape[0], 1)), Y[:, :-1]))\n","        # Y = Y[:,:,np.newaxis]\n","        return Y\n","\n","    def binarize_output_response(self, Y):\n","        return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n","                        for row in Y])\n","\n","    def respond_to_input(self, model, input_sent):\n","        input_y = self.include_start_token(self.PAD *np.ones((1, self.max_seq_len)))\n","        ids = np.array(self.words_to_indices(input_sent)).reshape((1, self.max_seq_len))\n","        for pos in range(self.max_seq_len - 1):\n","            pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n","            # pred = model.predict([ids, input_y])[0]\n","            input_y[:, pos + 1] = pred[pos]\n","        return self.indices_to_words(model.predict([ids, input_y]).argmax(axis=2)[0])\n","\n","    def train_model(self, model, X_train, X_test, y_train, y_test):\n","        input_y_train = self.include_start_token(y_train)\n","        print(input_y_train.shape)\n","        input_y_test = self.include_start_token(y_test)\n","        print(input_y_test.shape)\n","        early = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n","\n","        checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5', monitor='val_loss',\n","                                     verbose=1, save_best_only=True, mode='auto')\n","\n","        lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, mode='auto')\n","\n","        model.fit([X_train, input_y_train], y_train,\n","                   epochs=self.epochs,\n","                   batch_size=self.batch_size,\n","                   validation_data=([X_test, input_y_test], y_test),\n","                   callbacks=[early, checkpoint, lr_reduce],\n","                   shuffle=True)\n","\n","        return model\n","\n","    def generate_response(self, model, sentences):\n","        output_responses = []\n","        print(sentences)\n","        for sent in sentences:\n","            response = self.respond_to_input(model, sent)\n","            output_responses.append(response)\n","        out_df = pd.DataFrame()\n","        out_df['Tweet in'] = sentences\n","        out_df['Tweet out'] = output_responses\n","        return out_df\n","\n","    def main(self):\n","        if self.mode == 'train':\n","            X_train, X_test, y_train, y_test, test_sentences = self.data_creation()\n","            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","            print('Data Creation completed')\n","            model = self.create_model()\n","            print('Model creation completed')\n","            model = self.train_model(model, X_train, X_test, y_train, y_test)\n","            test_responses = self.generate_response(model, test_sentences)\n","            print(test_sentences)\n","            print(test_responses)\n","            pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv', index=False)\n","     \n","        elif self.mode == 'inference':\n","            #seq2seq model\n","            model = load_model(self.load_model_from)\n","            self.vocabulary = joblib.load(os.path.join(self.outpath, 'vocabulary.pkl'))\n","            self.reverse_vocabulary = joblib.load(os.path.join(self.outpath, 'reverse_vocabulary.pkl'))\n","            count_vectorizer = joblib.load(os.path.join(self.outpath, 'count_vectorizer.pkl'))\n","            self.analyzer = count_vectorizer.build_analyzer()\n","\n","            #load inference model\n","            inference_model = load_model(self.inference_load_model_from)\n","            inference_intents = json.loads(open(self.inference_load_intents_from, encoding='cp1252').read())\n","            inference_words = pickle.load(open(self.inference_load_words,'rb'))\n","            inference_classes = pickle.load(open(self.inference_load_classes,'rb'))\n","\n","            while True:\n","                try:\n","                    userText = request.args.get('msg')\n","                    response = self.respond_to_input(model, userText)\n","                    return str(response)\n","    \n","                except(KeyboardInterrupt, EOFError, SystemExit):\n","                    break\n","\n","@app.route(\"/\")\n","def home():\n","    return render_template(\"index.html\")\n","\n","@app.route(\"/get\")\n","def get_bot_response():\n","    obj = chatbot()\n","    obj.mode = 'inference'\n","    response = obj.main()\n","    return response\n","\n","app.run()\n","\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://5340-34-75-230-250.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [28/Sep/2021 17:34:08] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [28/Sep/2021 17:34:09] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [28/Sep/2021 17:34:09] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [28/Sep/2021 17:34:10] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [28/Sep/2021 17:34:10] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["(1, 30)\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [28/Sep/2021 17:34:36] \"\u001b[37mGET /get?msg=Can%20you%20hear%20me%3F HTTP/1.1\u001b[0m\" 200 -\n"]}]},{"cell_type":"markdown","metadata":{"id":"-FSek_u97lkg"},"source":[""]}]}